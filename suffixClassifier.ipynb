{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine ADJ/Verb based on Suffix\n",
    "We build a model that determines, based on the suffix, whether a word is an adjective or verb.\n",
    "\n",
    "Using just the suffix (e.g., last 3 characters), we have a fixed-size input which is convenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements for modules that we may need below.\n",
    "import dynet as dy\n",
    "import sys\n",
    "from random import shuffle\n",
    "\n",
    "# read in the words and set up the \"input vocabulary\" (in this case: all characters)\n",
    "data = []\n",
    "classes = []\n",
    "with open('verben.words') as f:\n",
    "    data.extend([(0, l.strip()) for l in f.readlines()])\n",
    "    classes.append('verb')\n",
    "with open('adjektive.words') as f:\n",
    "    data.extend([(1, l.strip()) for l in f.readlines()])\n",
    "    classes.append('adjective')\n",
    "\n",
    "characters = set(\"\".join(list([x[1] for x in data])))\n",
    "characters.add(\"<NONE>\") # special tag that we use to pad in too short words\n",
    "\n",
    "int2char = list(characters)\n",
    "char2int = {c:i for i,c in enumerate(characters)}\n",
    "\n",
    "VOCAB_SIZE = len(characters)\n",
    "CLASSES_SIZE = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about we take a look at our data now:\n",
    "data # this produces a lot of output: pairs of *class* and *example*\n",
    "# let's call the pairs of class+example *instance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far, words have different lengths which has the potential of making things difficult. In this example, we want to take our decisions based on the last 3 characters of a word. We'll hence shorten the data to contain only the last three characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 # might as well make this flexible\n",
    "shortened_data = []\n",
    "for (cls, word) in data:\n",
    "    suffix = list(word[-N:len(word)])\n",
    "    # now, if we're unlucky, then the word was too shard and we have to pad at the beginning\n",
    "    while len(suffix) < N:\n",
    "        suffix.insert(0, '<NONE>')\n",
    "    shortened_data.append((cls, suffix))\n",
    "data = shortened_data\n",
    "# now, you might want to take another look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(params, instance):\n",
    "    dy.renew_cg()\n",
    "    (cls, suffix) = instance\n",
    "    lookup = params[\"lookup\"]\n",
    "    inputs = [lookup[char2int[c]] for c in suffix]\n",
    "    inputVector = dy.concatenate(inputs)\n",
    "    R = dy.parameter(params[\"R\"])\n",
    "    bias = dy.parameter(params[\"bias\"])\n",
    "    output = R * inputVector + bias\n",
    "    loss = dy.pickneglogsoftmax(output, cls)\n",
    "    estimatedClass = max([(v,i) for (i,v) in enumerate(output.value())])[1]\n",
    "    isCorrect = estimatedClass == cls\n",
    "    return loss, isCorrect\n",
    "\n",
    "# train, and report correctness after each training iteration\n",
    "def train(params, data):\n",
    "    shuffle(data)\n",
    "    trainer = trainer_type(pc)\n",
    "    for i in range(ITERATIONS):\n",
    "        correct = 0\n",
    "        for instance in data:\n",
    "            loss, isCorrect = compute(params, instance)\n",
    "            correct += 1 if isCorrect else 0\n",
    "            loss_value = loss.value()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "        print(\"IT: {}, correct: {}\".format(i, correct/len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 40\n",
    "HIDDEN_DIM = INPUT_DIM * N\n",
    "\n",
    "ITERATIONS = 10\n",
    "\n",
    "pc = dy.ParameterCollection()\n",
    "params = {}\n",
    "params[\"lookup\"] = pc.add_lookup_parameters((VOCAB_SIZE, INPUT_DIM))\n",
    "params[\"R\"] = pc.add_parameters((CLASSES_SIZE, HIDDEN_DIM))\n",
    "params[\"bias\"] = pc.add_parameters((CLASSES_SIZE))\n",
    "\n",
    "trainer_type = dy.SimpleSGDTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(params, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, cls = compute(params, (0, list(\"ern\")))\n",
    "print(loss.value(), cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
