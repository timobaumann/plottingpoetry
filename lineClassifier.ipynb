{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify lines of poetry\n",
    "We will classify lines of poetry as to their literary style: sound poetry or more natural parlando style\n",
    "\n",
    "Lines are variable length, so we will use an LSTM to encode the line into a fixed-length representation and then add a classificatin layer on top of this fixed-length representation.\n",
    "\n",
    "The goal of the model is no more to re-create the line (i.e., to remember it as best as possible) but to support the classification. We hence won't train it to recreate the line and won't generate from it anymore. (But this could still be useful as a secondary task in multi-task learning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements for modules that we may need below.\n",
    "import dynet as dy\n",
    "import sys\n",
    "from random import shuffle\n",
    "\n",
    "# read in the words and set up the \"input vocabulary\" (in this case: all characters)\n",
    "data = []\n",
    "classes = []\n",
    "with open('parlando.lines') as f:\n",
    "    data.extend([(0, l.strip()) for l in f.readlines()])\n",
    "    classes.append('parlando')\n",
    "with open('soundpoetry.lines') as f:\n",
    "    data.extend([(1, l.strip()) for l in f.readlines()])\n",
    "    classes.append('soundpoetry')\n",
    "\n",
    "characters = set(\"\".join(list([x[1] for x in data])))\n",
    "characters.add(\"<EOS>\") # special tag that we use to signal end of sequence\n",
    "\n",
    "int2char = list(characters)\n",
    "char2int = {c:i for i,c in enumerate(characters)}\n",
    "\n",
    "VOCAB_SIZE = len(characters)\n",
    "CLASSES_SIZE = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(rnn, params, instance):\n",
    "    dy.renew_cg()\n",
    "    (cls, line) = instance\n",
    "    lookup = params[\"lookup\"]\n",
    "    line = [\"<EOS>\"] + list(line) + [\"<EOS>\"]\n",
    "    line = [char2int[c] for c in line]\n",
    "    s = rnn.initial_state()\n",
    "    for c in line:\n",
    "        s = s.add_input(lookup[c])\n",
    "    R = dy.parameter(params[\"R\"])\n",
    "    bias = dy.parameter(params[\"bias\"])\n",
    "    output = R * s.output() + bias\n",
    "    loss = dy.pickneglogsoftmax(output, cls)\n",
    "    estimatedClass = max([(v,i) for (i,v) in enumerate(output.value())])[1]\n",
    "    isCorrect = estimatedClass == cls\n",
    "    return loss, isCorrect\n",
    "\n",
    "# train, and report correctness after each training iteration\n",
    "def train(rnn, params, data):\n",
    "    shuffle(data)\n",
    "    trainer = trainer_type(pc)\n",
    "    for i in range(ITERATIONS):\n",
    "        correct = 0\n",
    "        for instance in data:\n",
    "            loss, isCorrect = compute(rnn, params, instance)\n",
    "            correct += 1 if isCorrect else 0\n",
    "            loss_value = loss.value()\n",
    "            loss.backward()\n",
    "            trainer.update()\n",
    "        print(\"IT: {}, correct: {}\".format(i, correct/len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 20\n",
    "\n",
    "INPUT_DIM = 20\n",
    "HIDDEN_DIM = 50\n",
    "LAYERS = 1\n",
    "\n",
    "#builder_type = dy.SimpleRNNBuilder\n",
    "builder_type = dy.LSTMBuilder\n",
    "#builder_type = dy.GRUBuilder\n",
    "\n",
    "pc = dy.ParameterCollection()\n",
    "rnn = builder_type(LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\n",
    "# add parameters for the hidden->output part for both lstm and srnn\n",
    "params = {}\n",
    "params[\"lookup\"] = pc.add_lookup_parameters((VOCAB_SIZE, INPUT_DIM))\n",
    "params[\"R\"] = pc.add_parameters((CLASSES_SIZE, HIDDEN_DIM))\n",
    "params[\"bias\"] = pc.add_parameters((CLASSES_SIZE))\n",
    "\n",
    "trainer_type = dy.SimpleSGDTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(rnn, params, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is still very basic (and we have not evaluated fairly...).\n",
    "Extensions would be:\n",
    "\n",
    "- two RNNs for bi-directional analysis (forwards and backwards)\n",
    "- learning auto-attention for the model to pick up what to pay attention to\n",
    "- using pre-trained lookup-embeddings and/or pre-train other parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
